{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 20:39:33.355238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/macbook/anaconda3/envs/sentiment-analysis-nlp/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load Model & Tools\n",
    "# -------------------------------\n",
    "with open(\"../models/lstm gru/initial/lstm_model_final.pkl\", \"rb\") as f_lstm:\n",
    "    model_lstm = pickle.load(f_lstm)\n",
    "\n",
    "with open(\"../models/lstm gru/initial/gru_model_final.pkl\", \"rb\") as f_gru:\n",
    "    model_gru = pickle.load(f_gru)\n",
    "\n",
    "with open(\"../models/lstm gru/initial/tokenizer_lstm.pkl\", \"rb\") as f_tok:\n",
    "    tokenizer = pickle.load(f_tok)\n",
    "\n",
    "with open(\"../models/lstm gru/initial/label_encoder_lstm.pkl\", \"rb\") as f_lbl:\n",
    "    label_encoder = pickle.load(f_lbl)\n",
    "\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2: Prediction Function\n",
    "# -------------------------------\n",
    "def predict_sentiment(texts, model):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len)\n",
    "    preds = model.predict(padded)\n",
    "    classes = np.argmax(preds, axis=1)\n",
    "    labels = label_encoder.inverse_transform(classes)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 3: Test Tweet\n",
    "# -------------------------------\n",
    "contoh_tweet = [\n",
    "    \"Saya kecewa berat dengan janji politik.\",\n",
    "    \"Debatnya sangat seru dan menarik.\",\n",
    "    \"Biasa saja, tidak terlalu mempengaruhi pilihan saya.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "📝 \"Saya kecewa berat dengan janji politik.\"\n",
      "  🔵 LSTM: netral\n",
      "  🟢 GRU : netral\n",
      "--------------------------------------------------\n",
      "📝 \"Debatnya sangat seru dan menarik.\"\n",
      "  🔵 LSTM: netral\n",
      "  🟢 GRU : netral\n",
      "--------------------------------------------------\n",
      "📝 \"Biasa saja, tidak terlalu mempengaruhi pilihan saya.\"\n",
      "  🔵 LSTM: netral\n",
      "  🟢 GRU : positif\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hasil_lstm = predict_sentiment(contoh_tweet, model_lstm)\n",
    "hasil_gru = predict_sentiment(contoh_tweet, model_gru)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Tampilkan Hasil\n",
    "# -------------------------------\n",
    "for i, teks in enumerate(contoh_tweet):\n",
    "    print(f\"📝 \\\"{teks}\\\"\")\n",
    "    print(f\"  🔵 LSTM: {hasil_lstm[i]}\")\n",
    "    print(f\"  🟢 GRU : {hasil_gru[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/envs/sentiment-analysis-nlp/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load .pkl\n",
    "with open(\"../models/lstm gru/hyperparameter tuning/best_model_lstm.pkl\", \"rb\") as f:\n",
    "    model_lstm = pickle.load(f)\n",
    "\n",
    "with open(\"../models/lstm gru/hyperparameter tuning/best_model_gru.pkl\", \"rb\") as f:\n",
    "    model_gru = pickle.load(f)\n",
    "\n",
    "with open(\"../models/lstm gru/initial/tokenizer_lstm.pkl\", \"rb\") as f_tok:\n",
    "    tokenizer = pickle.load(f_tok)\n",
    "\n",
    "with open(\"../models/lstm gru/initial/label_encoder_lstm.pkl\", \"rb\") as f_lbl:\n",
    "    label_encoder = pickle.load(f_lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi function\n",
    "def predict_sentiment(texts, model, max_len=50):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len)\n",
    "    preds = model.predict(padded)\n",
    "    classes = np.argmax(preds, axis=1)\n",
    "    labels = label_encoder.inverse_transform(classes)\n",
    "    return list(zip(texts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh input\n",
    "contoh = [\n",
    "    \"Saya kecewa dengan kebijakan ekonomi mereka.\",\n",
    "    \"Pemaparan visi misinya sangat jelas dan meyakinkan!\",\n",
    "    \"Saya masih ragu, netral dulu deh.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "LSTM 🧠 \"Saya kecewa dengan kebijakan ekonomi mereka.\" → netral\n",
      "LSTM 🧠 \"Pemaparan visi misinya sangat jelas dan meyakinkan!\" → netral\n",
      "LSTM 🧠 \"Saya masih ragu, netral dulu deh.\" → netral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step\n",
      "GRU 🧠 \"Saya kecewa dengan kebijakan ekonomi mereka.\" → netral\n",
      "GRU 🧠 \"Pemaparan visi misinya sangat jelas dan meyakinkan!\" → netral\n",
      "GRU 🧠 \"Saya masih ragu, netral dulu deh.\" → netral\n"
     ]
    }
   ],
   "source": [
    "# Prediksi\n",
    "for text, pred in zip(contoh, predict_sentiment(contoh, model_lstm)):\n",
    "    print(f\"LSTM 🧠 \\\"{pred[0]}\\\" → {pred[1]}\")\n",
    "\n",
    "for text, pred in zip(contoh, predict_sentiment(contoh, model_gru)):\n",
    "    print(f\"GRU 🧠 \\\"{pred[0]}\\\" → {pred[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load semua\n",
    "model_lstm = load_model(\"../models/lstm gru/hyperparameter tuning fasttext/lstm_fasttext_model.h5\")\n",
    "model_gru = load_model(\"../models/lstm gru/hyperparameter tuning fasttext/gru_fasttext_model.h5\")\n",
    "\n",
    "with open(\"../models/lstm gru/hyperparameter tuning fasttext/tokenizer_fasttext.pkl\", \"rb\") as f_tok:\n",
    "    tokenizer = pickle.load(f_tok)\n",
    "\n",
    "with open(\"../models/lstm gru/hyperparameter tuning fasttext/label_encoder_fasttext.pkl\", \"rb\") as f_lbl:\n",
    "    label_encoder = pickle.load(f_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi fungsi\n",
    "def predict_sentiment(texts, model, max_len=50):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len)\n",
    "    preds = model.predict(padded)\n",
    "    classes = np.argmax(preds, axis=1)\n",
    "    labels = label_encoder.inverse_transform(classes)\n",
    "    return list(zip(texts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh input\n",
    "sample = [\n",
    "    \"Saya sangat kagum dengan pidatonya.\",\n",
    "    \"Bingung mau pilih siapa, semua biasa saja.\",\n",
    "    \"Kebijakan ini tidak masuk akal sama sekali.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LSTM:\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x168996950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x168996950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "\"Saya sangat kagum dengan pidatonya.\" → netral\n",
      "\"Bingung mau pilih siapa, semua biasa saja.\" → negatif\n",
      "\"Kebijakan ini tidak masuk akal sama sekali.\" → netral\n",
      "\n",
      "🧠 GRU:\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x168996b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x168996b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\"Saya sangat kagum dengan pidatonya.\" → positif\n",
      "\"Bingung mau pilih siapa, semua biasa saja.\" → negatif\n",
      "\"Kebijakan ini tidak masuk akal sama sekali.\" → netral\n"
     ]
    }
   ],
   "source": [
    "# Output LSTM vs GRU\n",
    "print(\"🧠 LSTM:\")\n",
    "for text, label in predict_sentiment(sample, model_lstm):\n",
    "    print(f\"\\\"{text}\\\" → {label}\")\n",
    "\n",
    "print(\"\\n🧠 GRU:\")\n",
    "for text, label in predict_sentiment(sample, model_gru):\n",
    "    print(f\"\\\"{text}\\\" → {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
