{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Data Preprocessing Baseline Model (Logistic Regression and Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Preprocessing untuk Model Baseline\n",
    "# -------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"../dataset/dataset.csv\")  # Pastikan kolomnya: 'tweet', 'sentimen'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2: Setup Tools\n",
    "# -------------------------------\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "default_stopwords = set(stop_factory.get_stop_words())\n",
    "\n",
    "# Tambahan stopwords informal dan singkatan umum\n",
    "custom_stopwords = {\n",
    "    'dsb', 'dst', 'dll', 'ok', 'oh', 'ya', 'tuh', 'gitu', 'gimana', 'gmn', 'nggak', 'ga', 'gak',\n",
    "    'mau', 'aja', 'nih', 'sih', 'dong', 'deh', 'lah', 'nya', 'punya', 'buat', 'jd', 'krn',\n",
    "    'dr', 'dgn', 'lg', 'tp', 'trus', 'utk', 'pdhl', 'sm', 'sy', 'lgsg', 'blm', 'udh', 'tdk',\n",
    "    'bgt', 'smua', 'skrg', 'td', 'trs', 'cmn', 'tp', 'bkn', 'dl', 'gw', 'loe', 'elu', 'luh',\n",
    "    'gue', 'gw', 'gwa', 'lo', 'sama', 'sm', 'enak', 'cepat', 'terlalu', 'biasa', 'yuk', 'aja', 'nya', 'bro',\n",
    "    'sis', 'wkwk', 'haha', 'hehe', 'anjir', 'banget', 'bgt', 'bangsat', 'nggk', 'yaudah', 'yaudahh',\n",
    "    'okeh', 'sip', 'yoi', 'lah', 'coy', 'bodo', 'fix', 'relate', 'halah', 'lol', 'btw', 'makasih',\n",
    "    'makasi', 'thx', 'thanks', 'seharusnya', 'sebetulnya', 'pasti', 'setidaknya', 'saja',\n",
    "    'tentu', 'walau', 'tolong', 'apalagi', 'aplg', 'bagaimanapun', 'bgmnpn'\n",
    "}\n",
    "\n",
    "stopwords_id = default_stopwords.union(custom_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Emoji Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 3: Emoji Mapping\n",
    "# -------------------------------\n",
    "emoji_map = {\n",
    "    \"üò†\": \"marah\", \"üò°\": \"marah\",\n",
    "    \"üò¢\": \"sedih\", \"üò≠\": \"sedih\",\n",
    "    \"üòÇ\": \"lucu\", \"ü§£\": \"lucu\",\n",
    "    \"üòä\": \"senang\", \"üòÅ\": \"senang\", \"üòÉ\": \"senang\", \"üòÑ\": \"senang\",\n",
    "    \"üòç\": \"cinta\", \"‚ù§Ô∏è\": \"cinta\",\n",
    "    \"üòí\": \"kesal\", \"üòû\": \"kecewa\",\n",
    "    \"üòé\": \"bangga\", \"üëç\": \"setuju\", \"üëé\": \"tidak_setuju\"\n",
    "}\n",
    "\n",
    "def replace_emojis(text):\n",
    "    for emoji, word in emoji_map.items():\n",
    "        text = text.replace(emoji, f\" {word} \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Clean Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 4: Clean Function\n",
    "# -------------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = replace_emojis(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)   # hapus URL\n",
    "    text = re.sub(r\"@\\w+\", '', text)                      # hapus mention\n",
    "    text = re.sub(r\"#\", '', text)                         # hapus hashtag simbol\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)                   # hapus simbol non-alphanum\n",
    "    text = re.sub(r\"\\d+\", '', text)                       # hapus angka\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()              # hapus spasi ganda\n",
    "\n",
    "    # Stopword removal\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords_id]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # Stemming\n",
    "    text = stemmer.stem(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5 Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 5: Apply Preprocessing\n",
    "# -------------------------------\n",
    "df[\"cleaned_tweet\"] = df[\"tweet\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset berhasil dibersihkan dan disimpan sebagai cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 6: Simpan / Lanjut ke Modeling\n",
    "# -------------------------------\n",
    "df[[\"tweet\", \"sentimen\", \"cleaned_tweet\"]].to_csv(\"../dataset/cleaned_dataset_baseline_model.csv\", index=False)\n",
    "print(\"‚úÖ Dataset berhasil dibersihkan dan disimpan sebagai cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
